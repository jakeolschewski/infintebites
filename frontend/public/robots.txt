# robots.txt for https://infantbites.com
# Goal: Maximize SEO & crawl efficiency while protecting APIs
# Last updated: 2025-11-17

User-agent: *
# Protect non-HTML endpoints
Disallow: /api/

# --- Duplicate / tracking parameter control ---
# These rules help avoid wasting crawl budget on the same page with tracking params
Disallow: /*?*utm_source=
Disallow: /*?*utm_medium=
Disallow: /*?*utm_campaign=
Disallow: /*?*utm_term=
Disallow: /*?*utm_content=
Disallow: /*?*gclid=
Disallow: /*?*fbclid=
Disallow: /*?*ref=
Disallow: /*?*session=
Disallow: /*?*sessionid=

# --- Make sure resources needed for rendering are crawlable ---
# This helps Google and other modern bots fully render your pages
Allow: /*.js$
Allow: /*.css$
Allow: /*.png$
Allow: /*.jpg$
Allow: /*.jpeg$
Allow: /*.gif$
Allow: /*.webp$
Allow: /*.svg$
Allow: /*.woff$
Allow: /*.woff2$

# Fallback: allow everything else
Allow: /

# --- Sitemaps ---
Sitemap: https://infantbites.com/sitemap.xml
